{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8526de4c",
   "metadata": {},
   "source": [
    "## Params of trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e823b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8202] # trtexec --help\r\n",
      "=== Model Options ===\r\n",
      "  --uff=<file>                UFF model\r\n",
      "  --onnx=<file>               ONNX model\r\n",
      "  --model=<file>              Caffe model (default = no model, random weights used)\r\n",
      "  --deploy=<file>             Caffe prototxt file\r\n",
      "  --output=<name>[,<name>]*   Output names (it can be specified multiple times); at least one output is required for UFF and Caffe\r\n",
      "  --uffInput=<name>,X,Y,Z     Input blob name and its dimensions (X,Y,Z=C,H,W), it can be specified multiple times; at least one is required for UFF models\r\n",
      "  --uffNHWC                   Set if inputs are in the NHWC layout instead of NCHW (use X,Y,Z=H,W,C order in --uffInput)\r\n",
      "\r\n",
      "=== Build Options ===\r\n",
      "  --maxBatch                  Set max batch size and build an implicit batch engine (default = same size as --batch)\r\n",
      "                              This option should not be used when the input model is ONNX or when dynamic shapes are provided.\r\n",
      "  --minShapes=spec            Build with dynamic shapes using a profile with the min shapes provided\r\n",
      "  --optShapes=spec            Build with dynamic shapes using a profile with the opt shapes provided\r\n",
      "  --maxShapes=spec            Build with dynamic shapes using a profile with the max shapes provided\r\n",
      "  --minShapesCalib=spec       Calibrate with dynamic shapes using a profile with the min shapes provided\r\n",
      "  --optShapesCalib=spec       Calibrate with dynamic shapes using a profile with the opt shapes provided\r\n",
      "  --maxShapesCalib=spec       Calibrate with dynamic shapes using a profile with the max shapes provided\r\n",
      "                              Note: All three of min, opt and max shapes must be supplied.\r\n",
      "                                    However, if only opt shapes is supplied then it will be expanded so\r\n",
      "                                    that min shapes and max shapes are set to the same values as opt shapes.\r\n",
      "                                    Input names can be wrapped with escaped single quotes (ex: \\'Input:0\\').\r\n",
      "                              Example input shapes spec: input0:1x3x256x256,input1:1x3x128x128\r\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\r\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\r\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\r\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs.\r\n",
      "  --inputIOFormats=spec       Type and format of each of the input tensors (default = all inputs in fp32:chw)\r\n",
      "                              See --outputIOFormats help for the grammar of type and format list.\r\n",
      "                              Note: If this option is specified, please set comma-separated types and formats for all\r\n",
      "                                    inputs following the same order as network inputs ID (even if only one input\r\n",
      "                                    needs specifying IO format) or set the type and format once for broadcasting.\r\n",
      "  --outputIOFormats=spec      Type and format of each of the output tensors (default = all outputs in fp32:chw)\r\n",
      "                              Note: If this option is specified, please set comma-separated types and formats for all\r\n",
      "                                    outputs following the same order as network outputs ID (even if only one output\r\n",
      "                                    needs specifying IO format) or set the type and format once for broadcasting.\r\n",
      "                              IO Formats: spec  ::= IOfmt[\",\"spec]\r\n",
      "                                          IOfmt ::= type:fmt\r\n",
      "                                          type  ::= \"fp32\"|\"fp16\"|\"int32\"|\"int8\"\r\n",
      "                                          fmt   ::= (\"chw\"|\"chw2\"|\"chw4\"|\"hwc8\"|\"chw16\"|\"chw32\"|\"dhwc8\")[\"+\"fmt]\r\n",
      "  --workspace=N               Set workspace size in megabytes (default = 16)\r\n",
      "  --profilingVerbosity=mode   Specify profiling verbosity. mode ::= layer_names_only|detailed|none (default = layer_names_only)\r\n",
      "  --minTiming=M               Set the minimum number of iterations used in kernel selection (default = 1)\r\n",
      "  --avgTiming=M               Set the number of times averaged in each iteration for kernel selection (default = 8)\r\n",
      "  --refit                     Mark the engine as refittable. This will allow the inspection of refittable layers \r\n",
      "                              and weights within the engine.\r\n",
      "  --sparsity=spec             Control sparsity (default = disabled). \r\n",
      "                              Sparsity: spec ::= \"disable\", \"enable\", \"force\"\r\n",
      "                              Note: Description about each of these options is as below\r\n",
      "                                    disable = do not enable sparse tactics in the builder (this is the default)\r\n",
      "                                    enable  = enable sparse tactics in the builder (but these tactics will only be\r\n",
      "                                              considered if the weights have the right sparsity pattern)\r\n",
      "                                    force   = enable sparse tactics in the builder and force-overwrite the weights to have\r\n",
      "                                              a sparsity pattern (even if you loaded a model yourself)\r\n",
      "  --noTF32                    Disable tf32 precision (default is to enable tf32, in addition to fp32)\r\n",
      "  --fp16                      Enable fp16 precision, in addition to fp32 (default = disabled)\r\n",
      "  --int8                      Enable int8 precision, in addition to fp32 (default = disabled)\r\n",
      "  --best                      Enable all precisions to achieve the best performance (default = disabled)\r\n",
      "  --directIO                  Avoid reformatting at network boundaries. (default = disabled)\r\n",
      "  --precisionConstraints=spec Control precision constraints. (default = none)\r\n",
      "                                  Precision Constaints: spec ::= \"none\" | \"obey\" | \"prefer\"\r\n",
      "                                  none = no constraints\r\n",
      "                                  prefer = meet precision constraints if possible\r\n",
      "                                  obey = meet precision constraints or fail otherwise\r\n",
      "  --calib=<file>              Read INT8 calibration cache file\r\n",
      "  --safe                      Enable build safety certified engine\r\n",
      "  --consistency               Perform consistency checking on safety certified engine\r\n",
      "  --restricted                Enable safety scope checking with kSAFETY_SCOPE build flag\r\n",
      "  --saveEngine=<file>         Save the serialized engine\r\n",
      "  --loadEngine=<file>         Load a serialized engine\r\n",
      "  --tacticSources=tactics     Specify the tactics to be used by adding (+) or removing (-) tactics from the default \r\n",
      "                              tactic sources (default = all available tactics).\r\n",
      "                              Note: Currently only cuDNN, cuBLAS and cuBLAS-LT are listed as optional tactics.\r\n",
      "                              Tactic Sources: tactics ::= [\",\"tactic]\r\n",
      "                                              tactic  ::= (+|-)lib\r\n",
      "                                              lib     ::= \"CUBLAS\"|\"CUBLAS_LT\"|\"CUDNN\"\r\n",
      "                              For example, to disable cudnn and enable cublas: --tacticSources=-CUDNN,+CUBLAS\r\n",
      "  --noBuilderCache            Disable timing cache in builder (default is to enable timing cache)\r\n",
      "  --timingCacheFile=<file>    Save/load the serialized global timing cache\r\n",
      "\r\n",
      "=== Inference Options ===\r\n",
      "  --batch=N                   Set batch size for implicit batch engines (default = 1)\r\n",
      "                              This option should not be used when the engine is built from an ONNX model or when dynamic\r\n",
      "                              shapes are provided when the engine is built.\r\n",
      "  --shapes=spec               Set input shapes for dynamic shapes inference inputs.\r\n",
      "                              Note: Input names can be wrapped with escaped single quotes (ex: \\'Input:0\\').\r\n",
      "                              Example input shapes spec: input0:1x3x256x256, input1:1x3x128x128\r\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\r\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\r\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\r\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs.\r\n",
      "  --loadInputs=spec           Load input values from files (default = generate random inputs). Input names can be wrapped with single quotes (ex: 'Input:0')\r\n",
      "                              Input values spec ::= Ival[\",\"spec]\r\n",
      "                                           Ival ::= name\":\"file\r\n",
      "  --iterations=N              Run at least N inference iterations (default = 10)\r\n",
      "  --warmUp=N                  Run for N milliseconds to warmup before measuring performance (default = 200)\r\n",
      "  --duration=N                Run performance measurements for at least N seconds wallclock time (default = 3)\r\n",
      "  --sleepTime=N               Delay inference start with a gap of N milliseconds between launch and compute (default = 0)\r\n",
      "  --idleTime=N                Sleep N milliseconds between two continuous iterations(default = 0)\r\n",
      "  --streams=N                 Instantiate N engines to use concurrently (default = 1)\r\n",
      "  --exposeDMA                 Serialize DMA transfers to and from device (default = disabled).\r\n",
      "  --noDataTransfers           Disable DMA transfers to and from device (default = enabled).\r\n",
      "  --useManagedMemory          Use managed memory instead of seperate host and device allocations (default = disabled).\r\n",
      "  --useSpinWait               Actively synchronize on GPU events. This option may decrease synchronization time but increase CPU usage and power (default = disabled)\r\n",
      "  --threads                   Enable multithreading to drive engines with independent threads (default = disabled)\r\n",
      "  --useCudaGraph              Use CUDA graph to capture engine execution and then launch inference (default = disabled).\r\n",
      "                              This flag may be ignored if the graph capture fails.\r\n",
      "  --timeDeserialize           Time the amount of time it takes to deserialize the network and exit.\r\n",
      "  --timeRefit                 Time the amount of time it takes to refit the engine before inference.\r\n",
      "  --separateProfileRun        Do not attach the profiler in the benchmark run; if profiling is enabled, a second profile run will be executed (default = disabled)\r\n",
      "  --buildOnly                 Skip inference perf measurement (default = disabled)\r\n",
      "\r\n",
      "=== Build and Inference Batch Options ===\r\n",
      "                              When using implicit batch, the max batch size of the engine, if not given, \r\n",
      "                              is set to the inference batch size;\r\n",
      "                              when using explicit batch, if shapes are specified only for inference, they \r\n",
      "                              will be used also as min/opt/max in the build profile; if shapes are \r\n",
      "                              specified only for the build, the opt shapes will be used also for inference;\r\n",
      "                              if both are specified, they must be compatible; and if explicit batch is \r\n",
      "                              enabled but neither is specified, the model must provide complete static\r\n",
      "                              dimensions, including batch size, for all inputs\r\n",
      "                              Using ONNX models automatically forces explicit batch.\r\n",
      "\r\n",
      "=== Reporting Options ===\r\n",
      "  --verbose                   Use verbose logging (default = false)\r\n",
      "  --avgRuns=N                 Report performance measurements averaged over N consecutive iterations (default = 10)\r\n",
      "  --percentile=P              Report performance for the P percentage (0<=P<=100, 0 representing max perf, and 100 representing min perf; (default = 99%)\r\n",
      "  --dumpRefit                 Print the refittable layers and weights from a refittable engine\r\n",
      "  --dumpOutput                Print the output tensor(s) of the last inference iteration (default = disabled)\r\n",
      "  --dumpProfile               Print profile information per layer (default = disabled)\r\n",
      "  --dumpLayerInfo             Print layer information of the engine to console (default = disabled)\r\n",
      "  --exportTimes=<file>        Write the timing results in a json file (default = disabled)\r\n",
      "  --exportOutput=<file>       Write the output tensors to a json file (default = disabled)\r\n",
      "  --exportProfile=<file>      Write the profile information per layer in a json file (default = disabled)\r\n",
      "  --exportLayerInfo=<file>    Write the layer information of the engine in a json file (default = disabled)\r\n",
      "\r\n",
      "=== System Options ===\r\n",
      "  --device=N                  Select cuda device N (default = 0)\r\n",
      "  --useDLACore=N              Select DLA core N for layers that support DLA (default = none)\r\n",
      "  --allowGPUFallback          When DLA is enabled, allow GPU fallback for unsupported layers (default = disabled)\r\n",
      "  --plugins                   Plugin library (.so) to load (can be specified multiple times)\r\n",
      "\r\n",
      "=== Help ===\r\n",
      "  --help, -h                  Print this message\r\n"
     ]
    }
   ],
   "source": [
    "!trtexec --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d27752",
   "metadata": {},
   "source": [
    "## Onnx to TensoRT Engine with trtexec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0356ba",
   "metadata": {},
   "source": [
    "- TF32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d51b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/12/2022-23:51:41] [W] [TRT] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\r\n"
     ]
    }
   ],
   "source": [
    "!trtexec \\\n",
    "    --onnx=onnx_output.onnx \\\n",
    "    --minShapes=input:1x3x224x224 \\\n",
    "    --optShapes=input:1x3x224x224 \\\n",
    "    --maxShapes=input:8x3x224x224 \\\n",
    "    --workspace=1024 \\\n",
    "    --saveEngine=TF32.engine \\\n",
    "    --shapes=input:1x3x224x224 \\\n",
    "    --verbose \\\n",
    "    > result-TF32.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f047b",
   "metadata": {},
   "source": [
    "- FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ceb89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/12/2022-23:52:05] [W] [TRT] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\r\n"
     ]
    }
   ],
   "source": [
    "!trtexec \\\n",
    "    --onnx=onnx_output.onnx \\\n",
    "    --minShapes=input:1x3x224x224 \\\n",
    "    --optShapes=input:1x3x224x224 \\\n",
    "    --maxShapes=input:8x3x224x224 \\\n",
    "    --workspace=1024 \\\n",
    "    --noTF32 \\\n",
    "    --saveEngine=FP32.engine \\\n",
    "    --shapes=input:1x3x224x224 \\\n",
    "    --verbose \\\n",
    "    > result-FP32.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76447ef3",
   "metadata": {},
   "source": [
    "- FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb126f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/12/2022-23:52:26] [W] [TRT] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\r\n"
     ]
    }
   ],
   "source": [
    "!trtexec \\\n",
    "    --onnx=onnx_output.onnx \\\n",
    "    --minShapes=input:1x3x224x224 \\\n",
    "    --optShapes=input:1x3x224x224 \\\n",
    "    --maxShapes=input:8x3x224x224 \\\n",
    "    --workspace=1024 \\\n",
    "    --fp16 \\\n",
    "    --saveEngine=FP16.engine \\\n",
    "    --shapes=input:1x3x224x224 \\\n",
    "    --verbose \\\n",
    "    > result-FP16.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84069015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
